rook-ceph:
  currentNamespaceOnly: true

  logLevel: INFO

  resources:
    requests:
      cpu: 800m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 256Mi

  csi:
    enableCephfsDriver: false
    # -- Set logging level for cephCSI containers maintained by the cephCSI.
    # Supported values from 0 to 5. 0 for general useful logs, 5 for trace level verbosity.
    logLevel: 0
    # -- CEPH CSI RBD provisioner resource requirement list
    # csi-omap-generator resources will be applied only if `enableOMAPGenerator` is set to `true`
    # @default -- see values.yaml
    # TODO: adjust resources
    csiRBDProvisionerResource: |
      - name : csi-provisioner
        resource:
          requests:
            memory: 16Mi
            cpu: 10m
          limits:
            memory: 128Mi
            cpu: 200m
      - name : csi-resizer
        resource:
          requests:
            memory: 16Mi
            cpu: 10m
          limits:
            memory: 128Mi
            cpu: 200m
      - name : csi-attacher
        resource:
          requests:
            memory: 16Mi
            cpu: 20m
          limits:
            memory: 128Mi
            cpu: 400m
      - name : csi-snapshotter
        resource:
          requests:
            memory: 16Mi
            cpu: 20m
          limits:
            memory: 128Mi
            cpu: 400m
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 16Mi
            cpu: 10m
          limits:
            memory: 256Mi
            cpu: 500m

    # -- CEPH CSI RBD plugin resource requirement list
    # @default -- see values.yaml
    csiRBDPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            memory: 16Mi
            cpu: 10m
          limits:
            memory: 64Mi
            cpu: 100m
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 16Mi
            cpu: 10m
          limits:
            memory: 128Mi
            cpu: 500m

    pluginNodeAffinity: rook-ceph.grigri.cloud/client=true

    serviceMonitor:
      # -- Enable ServiceMonitor for Ceph CSI drivers
      enabled: true
      # -- Service monitor scrape interval
      interval: 30s

    kubeletDirPath: /var/lib/kubelet

  cephCommandsTimeoutSeconds: 60

  monitoring:
    # -- Enable monitoring. Requires Prometheus to be pre-installed.
    # Enabling will also create RBAC rules to allow Operator to create ServiceMonitors
    enabled: true

rook-ceph-cluster:
  toolbox:
    enabled: true
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 16Mi

  configOverride: |
    [global]
    osd_pool_default_size = 2
    osd_pool_default_min_size = 2

  monitoring:
    enabled: true
    createPrometheusRules: false

  cephClusterSpec:
    # workaround to change interval
    monitoring:
      enabled: true
      interval: 30s
    waitTimeoutForHealthyOSDInMinutes: 60
    mon:
      # Set the number of mons to be started. Generally recommended to be 3.
      # For highest availability, an odd number of mons should be specified.
      count: 3

    mgr:
      # When higher availability of the mgr is needed, increase the count to 2.
      # In that case, one mgr will be active and one in standby. When Ceph updates which
      # mgr is active, Rook will update the mgr services to match the active mgr.
      count: 1
      modules:
        # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
        # are already enabled by other settings in the cluster CR.
        - name: pg_autoscaler
          enabled: true

    dashboard:
      enabled: true
      urlPrefix: /
      ssl: true

    network:
      provider: host

    # enable log collector, daemons will log on files and rotate
    logCollector:
      enabled: false

    # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
    # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
    # tolerate taints with a key of 'storage-node'.
    placement:
      all:
      mon:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: storage-node
                    operator: In
                    values:
                      - "true"
      osd:
      mgr:
      cleanup:
    resources:
      mgr:
        requests:
          cpu: "125m"
          memory: "512Mi"
        limits:
          memory: "1219Mi"
      mon:
        requests:
          cpu: "49m"
          memory: "477Mi"
        limits:
          memory: "1059Mi"
      osd:
        requests:
          cpu: "400m"
          memory: "500Mi"
        limits:
          memory: "2Gi"
      crashcollector:
        limits:
          cpu: "500m"
          memory: "60Mi"
        requests:
          cpu: "10m"
          memory: "16Mi"
    storage:
      useAllNodes: false
      useAllDevices: false
      config:
        osdsPerDevice: "1"
      nodes:
        - name: k8s-odroid-hc4-1
          devices:
            - name: /dev/disk/by-id/ata-WDC_WD30EZRX-00DC0B0_WD-WMC1T2457392
            - name: /dev/disk/by-id/ata-SanDisk_SDSSDHII240G_170234400122
        - name: k8s-odroid-hc4-2
          devices:
            - name: /dev/disk/by-id/ata-WDC_WD30EZRX-00DC0B0_WD-WMC1T2292099
            - name: /dev/disk/by-id/ata-SanDisk_SDSSDHII240G_170235401310
        - name: k8s-odroid-hc4-3
          devices:
            - name: /dev/disk/by-id/ata-TOSHIBA_HDWQ140_Y8I5K0D2FAYG
            - name: /dev/disk/by-id/ata-SanDisk_SD6SB1M-128G-1006_141924401021

    disruptionManagement:
      # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
      # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
      # block eviction of OSDs by default and unblock them safely when drains are detected.
      managePodBudgets: true
      # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
      # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
      osdMaintenanceTimeout: 60

    healthCheck:
      livenessProbe:
        mgr:
          disabled: false
          # timeout causes dashboard restart
          probe:
            timeoutSeconds: 20
            initialDelaySeconds: 60
            periodSeconds: 30

  ingress:
    dashboard:
      ingressClassName: nginx-internal
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod-dns
        external-dns.alpha.kubernetes.io/enabled: "true"
        nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      host:
        name: &host rook-ceph.internal.grigri.cloud
        path: "/"
      tls:
        - secretName: rook-ceph-general-tls
          hosts:
            - *host

  cephBlockPools:
    - name: rook-ceph-blockpool-hdd
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      spec:
        failureDomain: host
        replicated:
          size: 2
        deviceClass: hdd
        # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
        # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
        # enableRBDStats: true
      storageClass:
        enabled: true
        name: rook-ceph-block-hdd
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        parameters:
          imageFormat: "2"
          # RBD image features, equivalent to OR'd bitfield value: 63
          # Available for imageFormat: "2". Older releases of CSI RBD
          # support only the `layering` feature. The Linux kernel (KRBD) supports the
          # full feature complement as of 5.4
          imageFeatures: layering

          # These secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
  cephFileSystems: []

  cephBlockPoolsVolumeSnapshotClass:
    enabled: true
    name: rook-ceph-block
    isDefault: false
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots for available configuration
    parameters: {}

  cephObjectStores:
    - name: eu-central-1a
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
          deviceClass: ssd
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          deviceClass: hdd
        preservePoolsOnDelete: false
        gateway:
          port: 80
          resources:
            limits:
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 200Mi
          # securePort: 443
          # sslCertificateRef:
          instances: 2
          placement:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-rgw
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        name: rook-ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          region: eu-central-1a
      ingress:
        enabled: true
        ingressClassName: nginx-internal
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-prod-dns
          external-dns.alpha.kubernetes.io/enabled: "true"
          nginx.ingress.kubernetes.io/proxy-body-size: 1g
        host:
          name: &host s3.internal.grigri.cloud
          path: "/"
        tls:
          - secretName: s3-general-tls
            hosts:
              - *host
